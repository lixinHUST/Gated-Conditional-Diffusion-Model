{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GateFusionModel(nn.Module):\n",
    "    def __init__(self, m=5, n=5, k=5):\n",
    "        super(GateFusionModel, self).__init__()\n",
    "        \n",
    "        # 假设输入向量的长度\n",
    "        self.input_dim_1 = 768\n",
    "        self.input_dim_2 = 67\n",
    "        self.m=m\n",
    "        self.n=n\n",
    "        self.k=k\n",
    "        # 1D卷积层\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=1, out_channels=m, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=1, out_channels=n, kernel_size=5,stride=1, padding=2)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "        nn.Linear(768 + 67, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 最终卷积层\n",
    "        self.conv1d_final = nn.Conv1d(in_channels=k, out_channels=2 * k, kernel_size=1)\n",
    "        self.linear = nn.Linear(768 + 67, 768)\n",
    "\n",
    "    def forward(self, c1, c2):\n",
    "        batch_size = c1.size(0)\n",
    "        # Step 1: Conv1d transformation\n",
    "        transformed_vector1 = self.conv1d_1(c1)  # (batch_size, m, 768)\n",
    "        transformed_vector2 = self.conv1d_2(c2)  # (batch_size, n, 67)\n",
    "        \n",
    "        # Step 2: Concatenate all m * n combinations\n",
    "        transformed_vector1 = transformed_vector1.unsqueeze(2).repeat(1, 1, self.n, 1)  # (batch_size, m, n, 768)\n",
    "        transformed_vector2 = transformed_vector2.unsqueeze(1).repeat(1, self.m, 1, 1)  # (batch_size, m, n, 67)\n",
    "        combined_features = torch.cat([transformed_vector1, transformed_vector2], dim=-1)  # (batch_size, m, n, 768+67)\n",
    "        combined_features = combined_features.view(batch_size, -1, 768 + 67)  # (batch_size, m*n, 768+67)\n",
    "\n",
    "        # Step 3: Compute weights using MLP\n",
    "        mlp_weights = self.mlp(combined_features)  # (batch_size, m*n, 1)\n",
    "        avg_weights = combined_features.mean(dim=-1, keepdim=True)  # (batch_size, m*n, 1)\n",
    "        weights = mlp_weights * avg_weights  # (batch_size, m*n, 1)\n",
    "\n",
    "        # Step 4: Combine scaled features and original features\n",
    "        res_features = combined_features + combined_features * weights  # (batch_size, m*n, 768+67)\n",
    "\n",
    "        # Step 5: Compute gating weights and select top-k\n",
    "        gating_weights = res_features.mean(dim=-1)  # (batch_size, m*n)\n",
    "        topk_values, topk_indices = torch.topk(gating_weights, self.k, dim=-1)  # (batch_size, k)\n",
    "\n",
    "        topk_res_features = torch.gather(\n",
    "            res_features, dim=1, index=topk_indices.unsqueeze(-1).expand(-1, -1, 768 + 67)\n",
    "        )  # (batch_size, k, 768+67)\n",
    "\n",
    "\n",
    "        # Step 6: Final convolution layer\n",
    "        final_output = self.conv1d_final(topk_res_features)  # (batch_size, 2*k, 768 + 67)\n",
    "        final_output = final_output.mean(dim=1, keepdim=True)  # (batch_size, 1, 768 + 67)\n",
    "        \n",
    "        return self.linear(final_output)\n",
    "\n",
    "f1=torch.randn(8, 1, 768)\n",
    "f2=torch.randn(8, 1, 67)\n",
    "gate_model=GateFusionModel()\n",
    "out=gate_model(f1,f2)\n",
    "print(out.shape)  # torch.Size([8, 1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6704\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([1, 67])\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "import pandas as pd\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,img_dir='DATA_FOLDER',train_val_test='train'):\n",
    "        train_val_test_dict = {'train': 'trainset_normalized.csv', 'val': 'valset_normalized.csv', 'test': 'testset_normalized.csv'}\n",
    "        self.labelfile = os.path.join('files',train_val_test_dict[train_val_test])\n",
    "        self.label_array=np.array(pd.read_csv(self.labelfile))\n",
    "        self.label_dict={}\n",
    "        for i in range(len(self.label_array)):\n",
    "            self.label_dict[self.label_array[i][0].replace('.nii.gz','.png')] = self.label_array[i][1:]\n",
    "       \n",
    "        self.cc_img_root = os.path.join(img_dir,'images',train_val_test)\n",
    "        self.images=os.listdir(self.cc_img_root)\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # 保持 128x128 大小\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5), (0.5)),\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),  \n",
    "        ])\n",
    "        self.type=type\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def get_random_box_mask(self, image):\n",
    "        y, x = image.shape[-2:]\n",
    "        mask = torch.zeros((1, y, x))\n",
    "\n",
    "        top = random.randint(0, y)\n",
    "        left = random.randint(0, x)\n",
    "        height = random.randint(0, y - top)\n",
    "        width = random.randint(0, x - left)\n",
    "\n",
    "        mask[:, top:top+height, left:left+width] = 1\n",
    "\n",
    "        return 1-mask\n",
    "    \n",
    "    def gaussian_smooth_and_normalize(self, hard_labels, sigma=1.0):\n",
    "        \"\"\"\n",
    "        Apply Gaussian smoothing to a segmentation mask with labels {0, 1, 2},\n",
    "        and normalize the resulting soft labels to the range [0, 1].\n",
    "        \n",
    "        Parameters:\n",
    "        - hard_labels (np.array): The hard label segmentation map with values {0, 1, 2}.\n",
    "        - sigma (float): Standard deviation of the Gaussian kernel for smoothing. \n",
    "        \n",
    "        Returns:\n",
    "        - soft_labels (np.array): Softened label map with smoothed boundaries, scaled to [0, 1].\n",
    "        \"\"\"\n",
    "        # Apply Gaussian filter to the hard labels\n",
    "        smoothed_labels = scipy.ndimage.gaussian_filter(hard_labels.astype(np.float32), sigma=sigma)\n",
    "        \n",
    "        # Normalize the smoothed values to the range [0, 1]\n",
    "        soft_labels = (smoothed_labels - smoothed_labels.min()) / (smoothed_labels.max() - smoothed_labels.min())\n",
    "        \n",
    "        return soft_labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cc_img_path = os.path.join(self.cc_img_root, self.images[idx])\n",
    "        cc_mask_path = cc_img_path.replace('images','masks')\n",
    "        cc_image= self.image_transform(Image.open(cc_img_path).convert(\"RGB\"))\n",
    "        cc_mask = self.mask_transform(Image.open(cc_mask_path).convert(\"L\"))\n",
    "        hardlabel=np.array(cc_mask)\n",
    "        softlabel=self.gaussian_smooth_and_normalize(hardlabel, sigma=1)\n",
    "        \n",
    "        data = {}\n",
    "        data[\"image_target\"] = cc_image\n",
    "        data[\"image_cond\"] = torch.FloatTensor(softlabel).unsqueeze(0).repeat(3,1,1)\n",
    "        data[\"mass_cond\"]= torch.FloatTensor((hardlabel==2).astype(np.float32)).unsqueeze(0).repeat(3,1,1)\n",
    "        if self.images[idx] in self.label_dict:\n",
    "            data[\"additional_feature\"]= torch.FloatTensor((self.label_dict[self.images[idx]]).astype(np.float32)).unsqueeze(0)\n",
    "        else:\n",
    "            data[\"additional_feature\"]= torch.FloatTensor((np.array([0]*67)).astype(np.float32)).unsqueeze(0)\n",
    "        return data\n",
    "    \n",
    "dataset=CustomDataset(img_dir='DATA_FOLDER',train_val_test='train')\n",
    "print(len(dataset))\n",
    "data=dataset[0]\n",
    "print(data[\"image_target\"].shape) # torch.Size([3, 256, 256])\n",
    "print(data[\"image_cond\"].shape) # torch.Size([3, 256, 256])\n",
    "print(data[\"mass_cond\"].shape) # torch.Size([3, 256, 256])\n",
    "print(data[\"additional_feature\"].shape) # torch.Size([1, 67])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
